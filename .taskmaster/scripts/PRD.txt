Product Requirements Document: AI Multi-Search Assistant (MVP)
Overview
A simple AI assistant that answers user questions by intelligently choosing between RAG search, database queries, or direct LLM responses, then provides human-like answers. Built as a demonstration of LLM tool orchestration for a job assignment.

Purpose: Show LLM-driven tool selection and response synthesis
Users: Assignment evaluators
Value: Single interface demonstrating agent reasoning and tool integration

Core Features
1. Simple Chat Interface
What: Basic React frontend with input field and conversation display

Why: Demonstrates full-stack integration

How: Simple form submission with response display

2. Agent Controller with Tool Selection
What: LLM decides whether to use RAG tool, DB tool, or no tool

Why: Shows intelligent agent reasoning and decision-making

How: Query → LLM tool selection → tool execution (if any) → LLM response synthesis

3. RAG Document Search Tool
What: Search through documents using semantic similarity

Why: Shows vector database and embedding integration

How: Dockling chunking → text-embedding-3-large → pgvector → retrieval

4. Database Query Tool
What: Query PostgreSQL database for structured data

Why: Demonstrates SQL generation and database integration

How: Convert natural language to SQL queries and return results

5. Direct LLM Response (No Tool)
What: LLM answers directly without using any tools

Why: Shows agent can handle general queries that don't need external data

How: LLM generates response from its own knowledge

6. Response Synthesis
What: LLM always generates final human-like response

Why: Ensures consistent, conversational output regardless of tool used

How: Pass tool results (or empty if no tool) to LLM for final formatting

User Experience
Flow: User question → Agent decides tool → Tool executes → LLM synthesizes → User sees natural response

Example Scenarios (from assignment):

"What is covered in the refund policy?" → RAG Tool → Synthesized answer

"What is the balance of user 123?" → DB Tool → Synthesized answer

"Tell me a joke about databases." → No Tool → Direct LLM response

UI: Input field, chat display, loading indicator

Technical Architecture
Components
Frontend: React with basic state management

Backend: FastAPI with virtual environment setup

Agent Controller: LLM-based tool selection logic

Tools: RAG (pgvector), Database (PostgreSQL), Direct LLM

AI Services: OpenAI GPT-4o and text-embedding-3-large

Agent Flow
text
1. Receive user query
2. LLM decides: RAG Tool | DB Tool | No Tool
3. Execute selected tool (if any)
4. LLM synthesizes final response
5. Return to user
Development Roadmap
Phase 1: Working MVP
Goal: Complete end-to-end system with all three response paths

Backend Setup:

Create Python virtual environment (python -m venv venv)

Install FastAPI, LangChain, psycopg2, OpenAI client

Set up PostgreSQL with pgvector extension

Load 5-10 sample documents for RAG

Core Implementation:

Agent controller with LLM tool selection

RAG tool with Dockling chunking and vector search

DB tool with basic SQL generation

Direct LLM response path

Response synthesis endpoint

Simple FastAPI endpoints

Frontend:

Basic React chat interface

API integration

Response display

Phase 2: Integration & Testing
Goal: All tools working together through agent controller

Tasks:

Test all three decision paths

Ensure LLM always synthesizes final response

Add basic error handling

Create sample database with test data

Phase 3: Polish & Documentation
Goal: Clean, demoable system

Tasks:

README with setup instructions

Virtual environment documentation

Example test queries

UI improvements

Logical Dependency Chain
Environment Setup (venv, PostgreSQL, pgvector) → Database schema & sample data

Individual Tools (RAG, DB, Direct LLM) → Agent controller integration

Complete Agent State Example
json
{
  "session_id": "abc123",
  "user_query": "What is the refund policy?",
  "tool_decision": "RAG Tool", 
  "tool_output": "Refunds are available within 30 days if the product is unused.",
  "final_response": "Our refund policy allows refunds within 30 days of purchase, if the product is unused.",
  "conversation_history": [
    {
      "query": "What is the refund policy?",
      "response": "Our refund policy allows refunds within 30 days of purchase, if the product is unused."
    }
  ]
}
Agent Controller → Frontend integration → Testing & documentation

Critical Path: Agent controller is the heart - must route correctly to tools and always synthesize responses

Risks and Mitigations
Technical Risks
Tool selection accuracy: Use clear prompts with examples for LLM tool selection

Response synthesis consistency: Always pass through LLM synthesis, even for direct responses

Environment setup: Document exact Python dependencies and PostgreSQL setup

Scope Risks
Assignment timeline: Focus on working demo over perfect code

Tool complexity: Start with simple implementations that demonstrate the concepts

Integration challenges: Build and test each tool individually before connecting

Appendix
Environment Setup Requirements
bash
# Backend setup
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install fastapi uvicorn langchain openai psycopg2-binary pgvector dockling
Agent Decision Logic
LLM prompt includes examples of when to use each tool

Always returns tool choice + reasoning

Response synthesis happens regardless of tool selection

Success Criteria
Agent correctly chooses tools based on query type

All three response paths work (RAG, DB, Direct LLM)

Every response goes through LLM synthesis for natural language output

System runs locally with clear setup instructions